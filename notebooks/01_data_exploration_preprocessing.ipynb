{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6308ce2",
   "metadata": {},
   "source": [
    "# Fake vs Real News Classification - Data Exploration & Preprocessing\n",
    "\n",
    "This notebook performs exploratory data analysis and preprocessing for the fake news classification project.\n",
    "\n",
    "## Project Overview\n",
    "- **Goal**: Build a classifier to distinguish between real (1) and fake (0) news\n",
    "- **Dataset**: News articles with labels, titles, full text content, subjects, and dates\n",
    "- **Approach**: Use NLP techniques to process text and train machine learning models\n",
    "- **Output**: Preprocessed data ready for model training\n",
    "\n",
    "## Dataset Structure\n",
    "- `label`: 0 = fake news, 1 = real news\n",
    "- `title`: Headline text\n",
    "- `text`: Full article content\n",
    "- `subject`: Article topic/category\n",
    "- `date`: Publication date\n",
    "\n",
    "## Workflow\n",
    "1. Load and explore the training dataset (`data.csv`)\n",
    "2. Perform data cleaning and preprocessing\n",
    "3. Apply text processing techniques (tokenization, TF-IDF, etc.)\n",
    "4. Split data into training and test sets (stratified by label)\n",
    "5. Save preprocessed data for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90b12e4",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5f9d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text processing\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    print(\"NLTK data downloaded successfully!\")\n",
    "except:\n",
    "    print(\"NLTK data download failed - please download manually if needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9b74dd",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9611395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset\n",
    "print(\"Loading training dataset...\")\n",
    "df = pd.read_csv('../dataset/data.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nColumn Names:\", df.columns.tolist())\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Display memory usage for large dataset\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08483261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "missing_info = df.isnull().sum()\n",
    "print(missing_info)\n",
    "\n",
    "# Check data distribution\n",
    "print(\"\\nLabel distribution (0=fake, 1=real):\")\n",
    "label_counts = df['label'].value_counts().sort_index()\n",
    "print(label_counts)\n",
    "\n",
    "# Calculate percentages\n",
    "print(\"\\nLabel distribution (percentages):\")\n",
    "label_pct = df['label'].value_counts(normalize=True).sort_index() * 100\n",
    "for label, pct in label_pct.items():\n",
    "    label_name = 'Fake' if label == 0 else 'Real'\n",
    "    print(f\"  {label_name} (Label {label}): {pct:.1f}%\")\n",
    "\n",
    "# Check subjects\n",
    "print(\"\\nSubject distribution:\")\n",
    "subject_counts = df['subject'].value_counts()\n",
    "print(subject_counts.head(10))  # Show top 10 subjects\n",
    "\n",
    "# Date range analysis\n",
    "if not df['date'].isnull().all():\n",
    "    print(\"\\nDate range:\")\n",
    "    try:\n",
    "        df['date_parsed'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "        if df['date_parsed'].notna().any():\n",
    "            print(f\"From: {df['date_parsed'].min()}\")\n",
    "            print(f\"To: {df['date_parsed'].max()}\")\n",
    "        else:\n",
    "            print(\"Date parsing failed - treating as string\")\n",
    "    except:\n",
    "        print(\"Date column contains mixed formats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a8bc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize label distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Label distribution - Bar plot\n",
    "label_counts.plot(kind='bar', ax=axes[0, 0], color=['red', 'green'])\n",
    "axes[0, 0].set_title('Distribution of Labels')\n",
    "axes[0, 0].set_xlabel('Label (0=Fake, 1=Real)')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Label distribution - Pie chart\n",
    "label_counts.plot(kind='pie', ax=axes[0, 1], autopct='%1.1f%%', \n",
    "                  labels=['Fake News', 'Real News'], colors=['red', 'green'])\n",
    "axes[0, 1].set_title('Label Distribution')\n",
    "axes[0, 1].set_ylabel('')\n",
    "\n",
    "# Subject distribution (top 10)\n",
    "subject_counts.head(10).plot(kind='bar', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Top 10 News Subjects')\n",
    "axes[1, 0].set_xlabel('Subject')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Label distribution by subject (for top subjects)\n",
    "top_subjects = subject_counts.head(5).index\n",
    "subject_label_df = df[df['subject'].isin(top_subjects)].groupby(['subject', 'label']).size().unstack(fill_value=0)\n",
    "subject_label_df.plot(kind='bar', ax=axes[1, 1], color=['red', 'green'], width=0.8)\n",
    "axes[1, 1].set_title('Label Distribution by Top Subjects')\n",
    "axes[1, 1].set_xlabel('Subject')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].legend(['Fake', 'Real'])\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "total_articles = len(df)\n",
    "fake_articles = len(df[df['label'] == 0])\n",
    "real_articles = len(df[df['label'] == 1])\n",
    "\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"Total articles: {total_articles:,}\")\n",
    "print(f\"Fake articles: {fake_articles:,} ({fake_articles/total_articles*100:.1f}%)\")\n",
    "print(f\"Real articles: {real_articles:,} ({real_articles/total_articles*100:.1f}%)\")\n",
    "print(f\"Number of subjects: {df['subject'].nunique()}\")\n",
    "print(f\"Articles with missing text: {df['text'].isnull().sum()}\")\n",
    "print(f\"Articles with missing titles: {df['title'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d560d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length analysis for title and text columns\n",
    "print(\"Analyzing text lengths...\")\n",
    "\n",
    "# Calculate text statistics\n",
    "df['title_length'] = df['title'].astype(str).str.len()\n",
    "df['text_length'] = df['text'].astype(str).str.len()\n",
    "df['title_word_count'] = df['title'].astype(str).str.split().str.len()\n",
    "df['text_word_count'] = df['text'].astype(str).str.split().str.len()\n",
    "\n",
    "# Statistics by label\n",
    "print(\"Text Statistics by Label (0=Fake, 1=Real):\")\n",
    "print(\"\\n1. Title Character Length:\")\n",
    "title_stats = df.groupby('label')['title_length'].describe()\n",
    "print(title_stats)\n",
    "\n",
    "print(\"\\n2. Article Text Character Length:\")\n",
    "text_stats = df.groupby('label')['text_length'].describe()\n",
    "print(text_stats)\n",
    "\n",
    "print(\"\\n3. Title Word Count:\")\n",
    "title_word_stats = df.groupby('label')['title_word_count'].describe()\n",
    "print(title_word_stats)\n",
    "\n",
    "print(\"\\n4. Article Text Word Count:\")\n",
    "text_word_stats = df.groupby('label')['text_word_count'].describe()\n",
    "print(text_word_stats)\n",
    "\n",
    "# Visualize text length distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Title lengths by label\n",
    "for label in [0, 1]:\n",
    "    label_name = 'Fake' if label == 0 else 'Real'\n",
    "    color = 'red' if label == 0 else 'green'\n",
    "    data = df[df['label'] == label]['title_length']\n",
    "    axes[0, 0].hist(data, bins=50, alpha=0.7, label=f'{label_name} News', color=color, density=True)\n",
    "axes[0, 0].set_title('Title Length Distribution')\n",
    "axes[0, 0].set_xlabel('Characters')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_xlim(0, 300)\n",
    "\n",
    "# Text lengths by label (log scale for better visualization)\n",
    "for label in [0, 1]:\n",
    "    label_name = 'Fake' if label == 0 else 'Real'\n",
    "    color = 'red' if label == 0 else 'green'\n",
    "    data = df[df['label'] == label]['text_length']\n",
    "    axes[0, 1].hist(np.log10(data + 1), bins=50, alpha=0.7, label=f'{label_name} News', color=color, density=True)\n",
    "axes[0, 1].set_title('Article Text Length Distribution (Log Scale)')\n",
    "axes[0, 1].set_xlabel('Log10(Characters + 1)')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Title word counts\n",
    "for label in [0, 1]:\n",
    "    label_name = 'Fake' if label == 0 else 'Real'\n",
    "    color = 'red' if label == 0 else 'green'\n",
    "    data = df[df['label'] == label]['title_word_count']\n",
    "    axes[1, 0].hist(data, bins=30, alpha=0.7, label=f'{label_name} News', color=color, density=True)\n",
    "axes[1, 0].set_title('Title Word Count Distribution')\n",
    "axes[1, 0].set_xlabel('Words')\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].set_xlim(0, 30)\n",
    "\n",
    "# Text word counts (log scale)\n",
    "for label in [0, 1]:\n",
    "    label_name = 'Fake' if label == 0 else 'Real'\n",
    "    color = 'red' if label == 0 else 'green'\n",
    "    data = df[df['label'] == label]['text_word_count']\n",
    "    axes[1, 1].hist(np.log10(data + 1), bins=50, alpha=0.7, label=f'{label_name} News', color=color, density=True)\n",
    "axes[1, 1].set_title('Article Word Count Distribution (Log Scale)')\n",
    "axes[1, 1].set_xlabel('Log10(Words + 1)')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa63c809",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2427df52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values and data cleaning\n",
    "print(\"Data Cleaning and Missing Value Handling\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Before cleaning\n",
    "print(\"BEFORE CLEANING:\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nMissing values:\")\n",
    "for col in df.columns:\n",
    "    missing_count = df[col].isnull().sum()\n",
    "    if missing_count > 0:\n",
    "        print(f\"  {col}: {missing_count} ({missing_count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Handle missing values strategically\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Drop rows where both title AND text are missing (unusable)\n",
    "unusable_rows = df_clean['title'].isnull() & df_clean['text'].isnull()\n",
    "df_clean = df_clean[~unusable_rows]\n",
    "print(f\"\\nRemoved {unusable_rows.sum()} rows with both title and text missing\")\n",
    "\n",
    "# Fill missing titles with \"No Title\"\n",
    "title_missing = df_clean['title'].isnull()\n",
    "df_clean.loc[title_missing, 'title'] = \"No Title\"\n",
    "print(f\"Filled {title_missing.sum()} missing titles\")\n",
    "\n",
    "# Fill missing text with \"No Content\"\n",
    "text_missing = df_clean['text'].isnull()\n",
    "df_clean.loc[text_missing, 'text'] = \"No Content\"\n",
    "print(f\"Filled {text_missing.sum()} missing text\")\n",
    "\n",
    "# Handle missing subjects\n",
    "subject_missing = df_clean['subject'].isnull()\n",
    "df_clean.loc[subject_missing, 'subject'] = \"Unknown\"\n",
    "print(f\"Filled {subject_missing.sum()} missing subjects\")\n",
    "\n",
    "# Handle missing dates\n",
    "date_missing = df_clean['date'].isnull()\n",
    "df_clean.loc[date_missing, 'date'] = \"Unknown Date\"\n",
    "print(f\"Filled {date_missing.sum()} missing dates\")\n",
    "\n",
    "# Remove any rows with missing labels (critical)\n",
    "label_missing = df_clean['label'].isnull()\n",
    "df_clean = df_clean[~label_missing]\n",
    "print(f\"Removed {label_missing.sum()} rows with missing labels\")\n",
    "\n",
    "# Final check\n",
    "print(f\"\\nAFTER CLEANING:\")\n",
    "print(f\"Dataset shape: {df_clean.shape}\")\n",
    "print(f\"Rows removed: {len(df) - len(df_clean)} ({(len(df) - len(df_clean))/len(df)*100:.1f}%)\")\n",
    "print(f\"Remaining missing values: {df_clean.isnull().sum().sum()}\")\n",
    "\n",
    "# Verify label distribution is maintained\n",
    "print(f\"\\nLabel distribution after cleaning:\")\n",
    "cleaned_label_counts = df_clean['label'].value_counts().sort_index()\n",
    "print(cleaned_label_counts)\n",
    "for label, count in cleaned_label_counts.items():\n",
    "    label_name = 'Fake' if label == 0 else 'Real'\n",
    "    print(f\"  {label_name}: {count:,} ({count/len(df_clean)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924292c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text data for NLP.\n",
    "    \n",
    "    Steps:\n",
    "    1. Convert to lowercase\n",
    "    2. Remove URLs, email addresses\n",
    "    3. Remove extra whitespace\n",
    "    4. Remove non-alphabetic characters (keeping spaces)\n",
    "    5. Strip leading/trailing whitespace\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove non-alphabetic characters but keep spaces\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply basic cleaning to title and text columns\n",
    "print(\"Applying text cleaning...\")\n",
    "\n",
    "# Clean titles and text\n",
    "df_clean['title_clean'] = df_clean['title'].apply(clean_text)\n",
    "df_clean['text_clean'] = df_clean['text'].apply(clean_text)\n",
    "\n",
    "# Create combined text (title + text) - this is often effective for classification\n",
    "df_clean['combined_text'] = df_clean['title_clean'] + ' ' + df_clean['text_clean']\n",
    "\n",
    "# Remove any completely empty combined text\n",
    "empty_combined = df_clean['combined_text'].str.strip() == ''\n",
    "print(f\"Removing {empty_combined.sum()} rows with empty combined text\")\n",
    "df_clean = df_clean[~empty_combined]\n",
    "\n",
    "print(\"Text cleaning completed!\")\n",
    "\n",
    "# Show examples of cleaning\n",
    "print(\"\\nExamples of text cleaning:\")\n",
    "print(\"=\" * 50)\n",
    "sample_idx = df_clean.index[0]\n",
    "print(\"Original title:\")\n",
    "print(f\"  '{df_clean.loc[sample_idx, 'title']}'\")\n",
    "print(\"Cleaned title:\")\n",
    "print(f\"  '{df_clean.loc[sample_idx, 'title_clean']}'\")\n",
    "print(\"\\nOriginal text (first 200 chars):\")\n",
    "print(f\"  '{df_clean.loc[sample_idx, 'text'][:200]}...'\")\n",
    "print(\"Cleaned text (first 200 chars):\")\n",
    "print(f\"  '{df_clean.loc[sample_idx, 'text_clean'][:200]}...'\")\n",
    "print(\"\\nCombined text (first 300 chars):\")\n",
    "print(f\"  '{df_clean.loc[sample_idx, 'combined_text'][:300]}...'\")\n",
    "\n",
    "# Check final text lengths after cleaning\n",
    "print(f\"\\nFinal dataset statistics:\")\n",
    "print(f\"Total articles: {len(df_clean):,}\")\n",
    "print(f\"Average combined text length: {df_clean['combined_text'].str.len().mean():.1f} characters\")\n",
    "print(f\"Average combined word count: {df_clean['combined_text'].str.split().str.len().mean():.1f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7f0243",
   "metadata": {},
   "source": [
    "## 4. Text Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c4683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced text processing with NLTK\n",
    "print(\"Applying advanced NLP preprocessing...\")\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def advanced_text_processing(text, use_stemming=True, remove_stopwords=True):\n",
    "    \"\"\"\n",
    "    Advanced text processing with tokenization, stopword removal, and stemming.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to process\n",
    "        use_stemming (bool): Whether to apply stemming\n",
    "        remove_stopwords (bool): Whether to remove stopwords\n",
    "    \n",
    "    Returns:\n",
    "        str: Processed text\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    # Tokenize the text\n",
    "    try:\n",
    "        tokens = word_tokenize(str(text))\n",
    "    except:\n",
    "        # Fallback to simple split if tokenization fails\n",
    "        tokens = str(text).split()\n",
    "    \n",
    "    # Convert to lowercase and remove very short tokens\n",
    "    tokens = [token.lower() for token in tokens if len(token) > 2]\n",
    "    \n",
    "    # Remove stopwords if requested\n",
    "    if remove_stopwords:\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Apply stemming if requested\n",
    "    if use_stemming:\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply advanced processing to combined text\n",
    "print(\"Processing combined text with NLTK...\")\n",
    "\n",
    "# Process in batches for large datasets\n",
    "batch_size = 1000\n",
    "processed_texts = []\n",
    "\n",
    "for i in range(0, len(df_clean), batch_size):\n",
    "    batch_end = min(i + batch_size, len(df_clean))\n",
    "    batch_texts = df_clean.iloc[i:batch_end]['combined_text'].tolist()\n",
    "    \n",
    "    batch_processed = [advanced_text_processing(text) for text in batch_texts]\n",
    "    processed_texts.extend(batch_processed)\n",
    "    \n",
    "    if (i // batch_size + 1) % 10 == 0:  # Progress update every 10 batches\n",
    "        print(f\"Processed {i + len(batch_processed):,} / {len(df_clean):,} articles...\")\n",
    "\n",
    "df_clean['text_processed'] = processed_texts\n",
    "\n",
    "print(\"Advanced text processing completed!\")\n",
    "\n",
    "# Remove articles with no processable text\n",
    "empty_processed = df_clean['text_processed'].str.strip() == ''\n",
    "if empty_processed.sum() > 0:\n",
    "    print(f\"Removing {empty_processed.sum()} articles with no processable text after NLP preprocessing\")\n",
    "    df_clean = df_clean[~empty_processed]\n",
    "\n",
    "# Show processing examples\n",
    "print(\"\\nExample of NLP processing:\")\n",
    "print(\"=\" * 50)\n",
    "sample_idx = df_clean.index[0]\n",
    "print(\"Before NLP processing:\")\n",
    "print(f\"  '{df_clean.loc[sample_idx, 'combined_text'][:200]}...'\")\n",
    "print(\"After NLP processing:\")\n",
    "print(f\"  '{df_clean.loc[sample_idx, 'text_processed'][:200]}...'\")\n",
    "\n",
    "# Final statistics\n",
    "print(f\"\\nFinal processed dataset:\")\n",
    "print(f\"Total articles: {len(df_clean):,}\")\n",
    "print(f\"Average processed text length: {df_clean['text_processed'].str.split().str.len().mean():.1f} words\")\n",
    "print(f\"Label distribution:\")\n",
    "final_label_counts = df_clean['label'].value_counts().sort_index()\n",
    "for label, count in final_label_counts.items():\n",
    "    label_name = 'Fake' if label == 0 else 'Real'\n",
    "    print(f\"  {label_name}: {count:,} ({count/len(df_clean)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504c3559",
   "metadata": {},
   "source": [
    "## 5. Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5995e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets (stratified split)\n",
    "print(\"Splitting data into training and testing sets...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Prepare features and target\n",
    "X = df_clean['text_processed']  # Processed text features\n",
    "y = df_clean['label']           # Target labels (0=fake, 1=real)\n",
    "\n",
    "# Additional features that could be useful\n",
    "additional_features = df_clean[['subject', 'title_length', 'text_length', 'title_word_count', 'text_word_count']].copy()\n",
    "\n",
    "print(f\"Total samples for splitting: {len(X):,}\")\n",
    "print(f\"Features: Processed text + {len(additional_features.columns)} additional features\")\n",
    "print(f\"Target distribution:\")\n",
    "for label, count in y.value_counts().sort_index().items():\n",
    "    label_name = 'Fake' if label == 0 else 'Real'\n",
    "    print(f\"  {label_name}: {count:,} ({count/len(y)*100:.1f}%)\")\n",
    "\n",
    "# Perform stratified train-test split\n",
    "# Using 80% for training, 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,           # 20% for testing\n",
    "    random_state=42,         # Fixed seed for reproducibility\n",
    "    stratify=y               # Maintain label distribution in both sets\n",
    ")\n",
    "\n",
    "# Also split additional features\n",
    "X_train_additional, X_test_additional, _, _ = train_test_split(\n",
    "    additional_features, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nSplit completed!\")\n",
    "print(f\"Training set: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Testing set: {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Verify label distribution is maintained in both sets\n",
    "print(f\"\\nLabel distribution in training set:\")\n",
    "train_label_counts = y_train.value_counts().sort_index()\n",
    "for label, count in train_label_counts.items():\n",
    "    label_name = 'Fake' if label == 0 else 'Real'\n",
    "    print(f\"  {label_name}: {count:,} ({count/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nLabel distribution in testing set:\")\n",
    "test_label_counts = y_test.value_counts().sort_index()\n",
    "for label, count in test_label_counts.items():\n",
    "    label_name = 'Fake' if label == 0 else 'Real'\n",
    "    print(f\"  {label_name}: {count:,} ({count/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "# Create directories for saving processed data\n",
    "import os\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "os.makedirs('../outputs', exist_ok=True)\n",
    "\n",
    "# Save the processed training and testing data\n",
    "train_data = pd.DataFrame({\n",
    "    'text_processed': X_train,\n",
    "    'label': y_train,\n",
    "    'subject': X_train_additional['subject'],\n",
    "    'title_length': X_train_additional['title_length'],\n",
    "    'text_length': X_train_additional['text_length']\n",
    "})\n",
    "\n",
    "test_data = pd.DataFrame({\n",
    "    'text_processed': X_test,\n",
    "    'label': y_test,\n",
    "    'subject': X_test_additional['subject'],\n",
    "    'title_length': X_test_additional['title_length'],\n",
    "    'text_length': X_test_additional['text_length']\n",
    "})\n",
    "\n",
    "# Save processed datasets\n",
    "train_data.to_csv('../data/processed/train.csv', index=False)\n",
    "test_data.to_csv('../data/processed/test.csv', index=False)\n",
    "\n",
    "# Also save the complete processed dataset\n",
    "df_clean_final = df_clean[[\n",
    "    'label', 'title', 'text', 'subject', 'date', \n",
    "    'title_clean', 'text_clean', 'combined_text', 'text_processed',\n",
    "    'title_length', 'text_length', 'title_word_count', 'text_word_count'\n",
    "]].copy()\n",
    "df_clean_final.to_csv('../data/processed/data_processed_full.csv', index=False)\n",
    "\n",
    "print(f\"\\nProcessed data saved:\")\n",
    "print(f\"  - Training set: ../data/processed/train.csv ({len(train_data):,} samples)\")\n",
    "print(f\"  - Testing set: ../data/processed/test.csv ({len(test_data):,} samples)\")\n",
    "print(f\"  - Full processed dataset: ../data/processed/data_processed_full.csv ({len(df_clean_final):,} samples)\")\n",
    "\n",
    "print(f\"\\nData preprocessing completed successfully!\")\n",
    "print(f\"Ready for model training in notebook 02.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
